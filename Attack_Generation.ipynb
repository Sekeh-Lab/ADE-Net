{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Of273bddN7p"
      },
      "source": [
        "#ADE-Net Attack Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code and Paper by: Nicholas Soucy and Dr. Salimeh Yasaei Sekeh"
      ],
      "metadata": {
        "id": "iDqjEqinB468"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns_AGmkodPJ6"
      },
      "source": [
        "##Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joRsUNRudSeo"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhtN518SeA6x"
      },
      "source": [
        "#### Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jjrlbrndU1s"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import layers\n",
        "from keras.callbacks import TensorBoard\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import h5py\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from scipy.special import softmax\n",
        "from scipy.stats import entropy\n",
        "import scipy.io as io\n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "import numpy as np\n",
        "import time\n",
        "import ipdb\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from collections import Counter\n",
        "from sklearn.metrics import cohen_kappa_score as kappa\n",
        "import keras.backend as K\n",
        "from keras.backend import clear_session\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "from keras.layers import Conv2DTranspose, Add, Input, Concatenate, Layer, SeparableConv2D\n",
        "from keras.layers import Dense, Activation, BatchNormalization, Dropout, LeakyReLU, Conv2D, Reshape\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from keras.layers import Conv3D, Conv3DTranspose, PReLU, BatchNormalization, MaxPool3D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.layers import Conv2DTranspose, Add, Input, Concatenate, Layer, SeparableConv2D\n",
        "from keras.layers import Dense, Activation, BatchNormalization, Dropout, LeakyReLU, Conv2D, Reshape\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "from sklearn import mixture\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "import sklearn.preprocessing as sp\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "import keras\n",
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import yaml\n",
        "import torch\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import Tensor\n",
        "from torch import optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization\n",
        "from keras.layers import Dropout, Input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from plotly.offline import init_notebook_mode\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import models\n",
        "import math\n",
        "import scipy.io as io\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "import torch.utils.data as data_utils\n",
        "import torchattacks\n",
        "from scipy.sparse.linalg import LinearOperator as ScipyLinearOperator\n",
        "from scipy.sparse.linalg import eigsh\n",
        "from warnings import warn\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "import art.attacks.evasion\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "np_config.enable_numpy_behavior()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cJsm1H3zJI3"
      },
      "outputs": [],
      "source": [
        "#GLOBAL VARIABLES TO CHANGE\n",
        "\n",
        "#Dataset Stuff\n",
        "dataset = [\"Indian_Pines\",\"Salinas\",\"Pavia_University\",\"Kennedy_Space_Center\",\"Botswana\",\"Houston\"]\n",
        "ds = 3\n",
        "\n",
        "#Batch Number\n",
        "batch = 1024\n",
        "\n",
        "#Which epsilon we wish to test\n",
        "epsilons = [0.1]\n",
        "epi = 0\n",
        "\n",
        "#Attack Stuff\n",
        "attack_types = ['FGSM','PGD','BIM','CW']\n",
        "num_attacks = 4\n",
        "\n",
        "#The amount of training for each attack for discriminator\n",
        "t_prime = 10\n",
        "\n",
        "if ds == 0:\n",
        "  PATH = \"Data/Indian_Pines/\"\n",
        "elif ds == 1:\n",
        "  PATH = \"Data/Salinas/\"\n",
        "elif ds == 2:\n",
        "  PATH = \"Data/Pavia_University/\"\n",
        "elif ds == 3:\n",
        "  PATH = \"Data/Kennedy_Space_Center/\"\n",
        "elif ds == 4:\n",
        "  PATH = \"Data/Botswana/\"\n",
        "elif ds == 5:\n",
        "  PATH = \"Data/Houston/\"\n",
        "\n",
        "print(\"PATH: \",PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvmS3bzdapp"
      },
      "source": [
        "### Dataset Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mwsJIdBndbI7"
      },
      "outputs": [],
      "source": [
        "#importing dataset functions\n",
        "\n",
        "def import_IP():\n",
        "  #Number of Classes = 16\n",
        "  #import indian Pines dataset\n",
        "  dataset = io.loadmat('Data/Indian_Pines/Indian_pines_corrected.mat')\n",
        "  data = dataset['indian_pines_corrected']\n",
        "\n",
        "  groundtruth = io.loadmat('Data/Indian_Pines/Indian_pines_gt.mat')\n",
        "  gt = groundtruth['indian_pines_gt']\n",
        "\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  X = x.reshape(dim_x, dim_y, dim_z)\n",
        "\n",
        "  return X, gt\n",
        "\n",
        "def import_Bot():\n",
        "  #Number of Classes = 14\n",
        "  #import Botswana dataset\n",
        "  dataset = io.loadmat('Data/Botswana/Botswana.mat')\n",
        "  data = dataset['Botswana']\n",
        "\n",
        "  groundtruth = io.loadmat('Data/Botswana/Botswana_gt.mat')\n",
        "  gt = groundtruth['Botswana_gt']\n",
        "\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  X = x.reshape(dim_x, dim_y, dim_z)\n",
        "\n",
        "  return X, gt\n",
        "\n",
        "def import_KSC():\n",
        "  #Number of classes = 13\n",
        "  #import Kennedy Space Center dataset\n",
        "  dataset = io.loadmat('Data/Kennedy_Space_Center/KSC.mat')\n",
        "  data = dataset['KSC']\n",
        "\n",
        "  groundtruth = io.loadmat('Data/Kennedy_Space_Center/KSC_gt.mat')\n",
        "  gt = groundtruth['KSC_gt']\n",
        "\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  X = x.reshape(dim_x, dim_y, dim_z)\n",
        "\n",
        "  return X, gt\n",
        "\n",
        "def import_Sal():\n",
        "  #Number of classes = 16\n",
        "  #import Salinas dataset\n",
        "  dataset = io.loadmat('Data/Salinas/Salinas_corrected.mat')\n",
        "  data = dataset['salinas_corrected']\n",
        "\n",
        "  groundtruth = io.loadmat('Data/Salinas/Salinas_gt.mat')\n",
        "  gt = groundtruth['salinas_gt']\n",
        "\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  X = x.reshape(dim_x, dim_y, dim_z)\n",
        "\n",
        "  return X, gt\n",
        "\n",
        "def import_PavU():\n",
        "  #Number of classes = 9\n",
        "  #import Pavia University dataset\n",
        "  dataset = io.loadmat('Data/Pavia_University/PaviaU.mat')\n",
        "  data = dataset['paviaU']\n",
        "\n",
        "  groundtruth = io.loadmat('Data/Pavia_University/PaviaU_gt.mat')\n",
        "  gt = groundtruth['paviaU_gt']\n",
        "\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  X = x.reshape(dim_x, dim_y, dim_z)\n",
        "\n",
        "  return X, gt\n",
        "\n",
        "def import_Houston():\n",
        "  #Number of Classes = 15\n",
        "  #import Houston dataset\n",
        "  dataset = io.loadmat('Data/Houston/Houston.mat')\n",
        "  data = dataset['Houston']\n",
        "\n",
        "  groundtruth = io.loadmat('Data/Houston/Houston_gt.mat')\n",
        "  gt = groundtruth['gt']\n",
        "\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  X = x.reshape(dim_x, dim_y, dim_z)\n",
        "\n",
        "  return X, gt\n",
        "\n",
        "\n",
        "#Data reshaping Functions\n",
        "\n",
        "def remove_background(data, gt):\n",
        "  tmp_a = []\n",
        "  for i in range(data.shape[0]):\n",
        "    if gt[i] == 0:\n",
        "      tmp_a.append(i)\n",
        "\n",
        "  x = np.delete(data,tmp_a,0)\n",
        "  y = np.delete(gt,tmp_a,0)\n",
        "  return x, y\n",
        "\n",
        "def flatten(data, gt):\n",
        "  #reshape data\n",
        "  dim_x, dim_y, dim_z = data.shape\n",
        "  dim = dim_x * dim_y\n",
        "\n",
        "  x = np.zeros(shape=(dim, dim_z))\n",
        "  y = gt\n",
        "\n",
        "\n",
        "  cont = 0\n",
        "  for i in range(dim_x):\n",
        "      for j in range(dim_y):\n",
        "          x[cont, :] = data[i, j, :]\n",
        "          cont += 1\n",
        "\n",
        "  y = np.reshape(y,(dim))\n",
        "\n",
        "\n",
        "  #normalize x data\n",
        "  scaler = sp.MinMaxScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BUbuJ2KWdYk7"
      },
      "outputs": [],
      "source": [
        "#functions and classes for our custom U-Net\n",
        "\n",
        "def process_data(data,gt,class_num = 16):\n",
        "\n",
        "  def load_patches(data, gt, patch_size = 1):\n",
        "\n",
        "      patches = []\n",
        "      patches_gt = []\n",
        "      for i in range(data.shape[0] - patch_size):\n",
        "        for j in range(data.shape[1] - patch_size):\n",
        "            patch = (data[i:i+patch_size, j:j+patch_size, :]).copy()\n",
        "            patch_gt = (gt[i:i+patch_size, j:j+patch_size]).copy()\n",
        "            if np.any(patch_gt == 0):\n",
        "                continue\n",
        "            else:\n",
        "                patches.append(patch)\n",
        "                patches_gt.append(patch_gt)\n",
        "      patches_1 = np.array(patches)\n",
        "      patches_gt = np.array(patches_gt) - 1\n",
        "      patches_gt_1 = to_categorical(patches_gt, num_classes = class_num)\n",
        "      patches_gt_2 = patches_gt_1.reshape(patches_gt_1.shape[0],1,1,patches_gt_1.shape[2])\n",
        "      return patches_1, patches_gt_2\n",
        "\n",
        "  def load_synthetic_patches(data, gt, patch_size = 1, small_patch_size = 1, \n",
        "                            oversample = 12, label_choice = 8):\n",
        "\n",
        "      patches_small = [[] for _ in range(class_num)] #16 classes\n",
        "      patches_gt_small = [[] for _ in range(class_num)]\n",
        "\n",
        "      for i in range(data.shape[0] - small_patch_size):\n",
        "        for j in range(data.shape[1] - small_patch_size):\n",
        "            patch = (data[i:i+small_patch_size, j:j+small_patch_size, :]).copy()\n",
        "            patch_gt = (gt[i:i+small_patch_size, j:j+small_patch_size]).copy()\n",
        "            if np.any(patch_gt == 0):\n",
        "                continue\n",
        "            else:\n",
        "                index = patch_gt[0,0] - 1\n",
        "                patches_small[index].append(patch)\n",
        "                patches_gt_small[index].append(patch_gt)\n",
        "\n",
        "      patches_small = [np.array(patches_small[i]) for i in range(class_num)]\n",
        "      patches_gt_small = [(np.array(patches_gt_small[i]) - 1) for i in range(class_num)]\n",
        "\n",
        "      ## Mixed patches\n",
        "\n",
        "      patches = []\n",
        "      patches_gt = []\n",
        "\n",
        "      for sample in range(int(oversample)):\n",
        "        new_patch = np.zeros((patch_size, patch_size, data.shape[2]))\n",
        "        new_patch_gt = np.zeros((patch_size, patch_size))\n",
        "                \n",
        "        for i in range(0, patch_size, small_patch_size):\n",
        "            for j in range(0, patch_size, small_patch_size):\n",
        "            \n",
        "                index_choice = np.random.randint(int(len(patches_small[label_choice]) * 0.75))\n",
        "                new_patch[i:i+small_patch_size, j:j+small_patch_size, :] = patches_small[label_choice][index_choice]\n",
        "                new_patch_gt[i:i+small_patch_size, j:j+small_patch_size] = patches_gt_small[label_choice][index_choice]            \n",
        "\n",
        "        patches.append(new_patch)\n",
        "        patches_gt.append(new_patch_gt)\n",
        "\n",
        "      patches = np.array(patches)\n",
        "      patches_gt = np.array(patches_gt)\n",
        "      patches_gt = to_categorical(patches_gt, num_classes=class_num)\n",
        "\n",
        "      return patches, patches_gt\n",
        "\n",
        "  for i in range(data.shape[-1]):\n",
        "    data[:,:,i] = (data[:,:,i] - np.mean(data[:,:,i])) / np.std(data[:,:,i])\n",
        "  train_patches, train_patches_gt = load_patches(data, gt)\n",
        "  train_patches_synthetic, train_patches_gt_synthetic = load_synthetic_patches(data, gt)\n",
        "  train_patches_gt_synthetic = train_patches_gt_synthetic.reshape(train_patches_gt_synthetic.shape[0],1,1,train_patches_gt_synthetic.shape[2])\n",
        "  train_patches = np.concatenate((train_patches, train_patches_synthetic), axis=0)\n",
        "  train_patches_gt = np.concatenate((train_patches_gt, train_patches_gt_synthetic), axis=0)\n",
        "\n",
        "  return (train_patches, train_patches_gt)\n",
        "\n",
        "class PixelSoftmax(Layer):\n",
        "    \"\"\"\n",
        "    Pixelwise Softmax for Semantic Segmentation. Also known as\n",
        "    4D Softmax in some sources. Applies Softmax along the last\n",
        "    axis (-1 axis). \n",
        "    \"\"\"\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis=axis\n",
        "        super(PixelSoftmax, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config().copy()\n",
        "      return config\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x,mask=None):\n",
        "        e = K.exp(x - K.max(x, axis=self.axis, keepdims=True))\n",
        "        s = K.sum(e, axis=self.axis, keepdims=True)\n",
        "        return e / s\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class statsLogger(Callback):\n",
        "    \"\"\"\n",
        "    Saving loss and accuracy details to an array\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.logs = []\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        logs['epoch'] = epoch\n",
        "        self.logs.append(logs)\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op4vkukwdfD3"
      },
      "source": [
        "## Feature Reduction Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkpqZiwzdfdf"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dqkGZrRhditB"
      },
      "outputs": [],
      "source": [
        "#PCA\n",
        "\n",
        "def PCA_DR(X,n):\n",
        "  #PCA\n",
        "  pca = PCA(n_components = n)\n",
        "\n",
        "  principalComponents = pca.fit_transform(X)\n",
        "\n",
        "  ev=pca.explained_variance_ratio_\n",
        "\n",
        "  return principalComponents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iASTqptrdlIT"
      },
      "source": [
        "## Classification Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Xt7DmMdx7e"
      },
      "source": [
        "### Unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNkb820vL5GF"
      },
      "source": [
        "#### Pytorch Unet Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1A6Pf7Kalr"
      },
      "source": [
        "Actual Custom Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0za-X04qKZq6"
      },
      "outputs": [],
      "source": [
        "class CustomNeuralNetwork(nn.Module):\n",
        "    def __init__(self,in_shape,class_num,want_logits=True):\n",
        "        super().__init__()\n",
        "        # Define Variables\n",
        "        self.input_shape = in_shape\n",
        "        self.class_num = class_num\n",
        "        self.want_logits = want_logits\n",
        "        # Define Layers:\n",
        "        # NOTE: Change kernel size to be the size of the patch\n",
        "        self.Conv1 = nn.Conv2d(self.input_shape, 64,kernel_size=(1,1), stride=(1,1), padding='same', bias=False)\n",
        "        self.bn1 = nn.InstanceNorm1d(64)\n",
        "        self.Conv2 = nn.Conv2d(64, 128,kernel_size=(1,1), stride=(2,2), bias=False)\n",
        "        self.bn2 = nn.InstanceNorm1d(128)\n",
        "        self.Conv3 = nn.Conv2d(128, 256,kernel_size=(1,1), stride=(2,2), bias=False)\n",
        "        self.bn3 = nn.InstanceNorm1d(256)\n",
        "        self.deco1 = nn.ConvTranspose2d(256,256,kernel_size=(1,1), stride=(1,1), bias=False)\n",
        "        self.deco2 = nn.ConvTranspose2d(384,128,kernel_size=(1,1), stride=(1,1), bias=False)\n",
        "        self.deco3 = nn.ConvTranspose2d(192,class_num,kernel_size=(1,1), stride=(1,1))\n",
        "\n",
        "        # Define Activation functions:\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.Lrelu = nn.LeakyReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Layers: 4\n",
        "        Activation Functions:\n",
        "        RELU for first two layers\n",
        "        Sigmoid for third layer\n",
        "        Log Softmax for last layer\n",
        "        \"\"\"\n",
        "        # import pdb;pdb.set_trace()\n",
        "        x = self.Conv1(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn1(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        op1 = x\n",
        "\n",
        "        x = self.Conv2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        op2 = x\n",
        "\n",
        "        x = self.Conv3(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn3(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        op3 = x\n",
        "\n",
        "        x = self.deco1(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn3(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = torch.cat((x, op2), dim=1)\n",
        "\n",
        "        x = self.deco2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = torch.cat((x, op1), dim=1)\n",
        "\n",
        "        x = self.deco3(x)\n",
        "        x = torch.reshape(x, (x.shape[0], self.class_num))\n",
        "\n",
        "        if(self.want_logits):\n",
        "          return x\n",
        "        else:\n",
        "          x = self.softmax(x)\n",
        "          return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZV4zgrQL9Em"
      },
      "source": [
        "#### Adversarial Generation Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flQE2NzmLrxP"
      },
      "source": [
        "Original network for adversairal generation via categorical cross entropy class loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bgF1LfkutD4Z"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "  \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "  maxk = max(topk)\n",
        "  batch_size = target.size(0)\n",
        "\n",
        "  _, pred = output.topk(maxk, 1, True, True)\n",
        "  pred = pred.t()\n",
        "  correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "  res = []\n",
        "  for k in topk:\n",
        "      correct_k = correct[:k].view(-1).float().sum(0)\n",
        "      res.append(correct_k.mul_(100.0 / batch_size))\n",
        "  return res\n",
        "\n",
        "\n",
        "def UNet_Conv_torch(train_loader, test_loader=None, num_epochs = 25, class_num = 16, want_logits = True, return_loss = False, return_train_acc = False, trained_model = None):\n",
        "\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    in_shape = images.shape[1]\n",
        "  \n",
        "  if trained_model == None:\n",
        "    NN = CustomNeuralNetwork(in_shape,class_num)  # Intialize you NN\n",
        "  else:\n",
        "    NN = trained_model\n",
        "  NN = NN.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(NN.parameters(), lr = 0.001)\n",
        "  min_train_loss = 10000000\n",
        "\n",
        "  for epoch in trange(num_epochs,desc=\"Training Pytorch U-Net\"):\n",
        "      train_running_loss = 0.0\n",
        "      train_acc = 0.0\n",
        "\n",
        "      model = NN.train()\n",
        "\n",
        "      ## training step\n",
        "      for idx, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "          images = images.clone().detach().requires_grad_(True)\n",
        "          labels = labels.clone().detach().requires_grad_(True)\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          ## forward + backprop + loss\n",
        "          output = model(images)\n",
        "          loss = criterion(output, labels)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "\n",
        "          ## update model params\n",
        "          optimizer.step()\n",
        "\n",
        "          train_running_loss += loss.detach().item()\n",
        "\n",
        "          # _, predicted = torch.max(output.data, 1)\n",
        "          # total = labels.size(0)\n",
        "          # correct = (predicted == labels).sum().item()\n",
        "          # train_acc = (100 * correct / total)\n",
        "      \n",
        "      model.eval()\n",
        "      print('Epoch: %d | Loss: %.4f' \\\n",
        "            %(epoch, train_running_loss / idx))  \n",
        "      if (train_running_loss / idx) < min_train_loss:\n",
        "        min_train_loss = (train_running_loss / idx)\n",
        "\n",
        "\n",
        "\n",
        "  # Accuracy on Test Data\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  if test_loader != None:\n",
        "    loader = test_loader\n",
        "  else:\n",
        "    loader = train_loader\n",
        "    flag = True\n",
        "  with torch.no_grad():\n",
        "      for idx, (images, labels) in enumerate(loader):\n",
        "\n",
        "          images = images.clone().detach().requires_grad_(True)\n",
        "          if flag:\n",
        "            labels = torch.tensor(np.array(tf.math.argmax(labels.numpy(), axis=1)))\n",
        "          else:\n",
        "            labels = labels.clone().detach().requires_grad_(True)\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          output = model(images)\n",
        "          # import pdb;pdb.set_trace()\n",
        "          # acc = accuracy(output, labels)\n",
        "          _, predicted = torch.max(output.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  # return(acc,model)\n",
        "  if return_loss and return_train_acc:\n",
        "    return((100 * correct / total),model,loss,min_train_loss)\n",
        "  else:\n",
        "    return((100 * correct / total),model,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MEmo9bVZw-S"
      },
      "source": [
        "## Training for Adversarial Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiXhLK2nQ9sf"
      },
      "source": [
        "Train U-Net for Adversarial Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ds == 0:\n",
        "  X, Y = import_IP()\n",
        "  cls = 16\n",
        "  en_type = 'k'\n",
        "  clusters = 2\n",
        "  print(dataset[ds])\n",
        "  print(X.shape, Y.shape)\n",
        "elif ds == 1:\n",
        "  X, Y = import_Sal()\n",
        "  cls = 16\n",
        "  en_type = 'g'\n",
        "  clusters = 3\n",
        "  print(dataset[ds])\n",
        "  print(X.shape, Y.shape)\n",
        "elif ds == 2: \n",
        "  X, Y = import_PavU()\n",
        "  cls = 9\n",
        "  en_type = 'g'\n",
        "  clusters = 2\n",
        "  print(dataset[ds])\n",
        "  print(X.shape, Y.shape)\n",
        "elif ds == 3:\n",
        "  X, Y = import_KSC()\n",
        "  cls = 13\n",
        "  en_type = 'k'\n",
        "  clusters = 2\n",
        "  print(dataset[ds])\n",
        "  print(X.shape, Y.shape)\n",
        "elif ds == 4:\n",
        "  X, Y = import_Bot()\n",
        "  cls = 14\n",
        "  en_type = 'k'\n",
        "  clusters = 3\n",
        "  print(dataset[ds])\n",
        "  print(X.shape, Y.shape)\n",
        "elif ds == 5:\n",
        "  X, Y = import_Houston()\n",
        "  cls = 15\n",
        "  print(dataset[ds])\n",
        "  print(X.shape, Y.shape)\n",
        "\n",
        "#Apply PCA\n",
        "X1 = X.shape[0]\n",
        "X2 = X.shape[1]\n",
        "X_f = X.reshape(X.shape[0]*X.shape[1],X.shape[-1])\n",
        "X = PCA_DR(X_f,30)\n",
        "X = X.reshape(X1,X2,X.shape[-1])\n",
        "\n",
        "#Process data for U-Net\n",
        "data, gt = process_data(X, Y, class_num = cls)\n",
        "print(data.shape, gt.shape)\n",
        "\n",
        "#convert data to correct shape\n",
        "gt = np.reshape(gt, (-1, cls))\n",
        "data = data.reshape(data.shape[0],data.shape[-1],data.shape[1],data.shape[2])"
      ],
      "metadata": {
        "id": "02CSQ3KpKbMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6DXGoeYzvmY"
      },
      "outputs": [],
      "source": [
        "#convert to Tensors\n",
        "gt = torch.Tensor(np.array(gt))\n",
        "data = torch.Tensor(np.array(data))\n",
        "\n",
        "#create datasets\n",
        "dataset = data_utils.TensorDataset(data, gt)\n",
        "train_loader = data_utils.DataLoader(dataset, batch_size=batch, shuffle=False,pin_memory=True)\n",
        "# dataset2 = data_utils.TensorDataset(x_test, y_test)\n",
        "# test_loader = data_utils.DataLoader(dataset2, batch_size=batch, shuffle=False,pin_memory=True)\n",
        "\n",
        "\n",
        "acc, model, optimizer = UNet_Conv_torch(train_loader, num_epochs = 150, class_num = cls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AA5lnX-N66y"
      },
      "outputs": [],
      "source": [
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5vCNVc-d_Wa"
      },
      "source": [
        "## Adversarial Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkeMJ95nDXs7"
      },
      "source": [
        "Function to apply attacks to a range of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T1YA39qJZwuo"
      },
      "outputs": [],
      "source": [
        "def Apply_Attacks(data,gt,model,optimizer,a_type='FGSM',cls = 16):\n",
        "\n",
        "  data = np.array(data)\n",
        "  print(\"gt.shape: \",gt.shape)\n",
        "  gt = np.argmax(np.array(gt),axis=1)\n",
        "  print(\"gt.shape now: \",gt.shape)\n",
        "\n",
        "  classifier = PyTorchClassifier(model=model, clip_values=(0.0, 1.0), loss=nn.CrossEntropyLoss(), optimizer=optimizer, input_shape=(30, 1, 1), nb_classes=cls)\n",
        "            \n",
        "  if a_type == 'FGSM':\n",
        "    atk = art.attacks.evasion.FastGradientMethod(estimator=classifier, eps=.1)\n",
        "  elif a_type == 'BIM':\n",
        "    atk = art.attacks.evasion.BasicIterativeMethod(estimator=classifier, eps=.1)\n",
        "  elif a_type == 'PGD':\n",
        "    atk = art.attacks.evasion.ProjectedGradientDescentPyTorch(estimator=classifier, eps=.1)\n",
        "  elif a_type == 'CW':\n",
        "    atk = art.attacks.evasion.CarliniL2Method(classifier=classifier)\n",
        "\n",
        "  perturbations = atk.generate(data)\n",
        "  perturbations = np.asarray(perturbations)\n",
        "    \n",
        "  return perturbations, gt\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QXJy__DcHj"
      },
      "source": [
        "Attack the full x_test an 'num_attacks' amount of times and put results into array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh0hUa57Dan8"
      },
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "print(gt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXPy4B_53AnM"
      },
      "outputs": [],
      "source": [
        "attacks_used = [attack_types[0],attack_types[1],attack_types[2],attack_types[3]]\n",
        "names = []\n",
        "data_att = [None] * num_attacks\n",
        "gt_att = [None] * num_attacks\n",
        "\n",
        "for i in range(num_attacks):\n",
        "  print(\"On Number \",(i+1))\n",
        "\n",
        "  data_att[i], gt_att[i] = Apply_Attacks(data.cpu().detach().numpy(),gt.cpu().detach().numpy(),model,optimizer,a_type=attacks_used[i],cls = cls)\n",
        "\n",
        "  names.append(attacks_used[i])\n",
        "  print(names[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_attacks):\n",
        "  print(data_att[i].shape)"
      ],
      "metadata": {
        "id": "i6IZGF_boja4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "attack_types = ['VANILA','FGSM','PGD','BIM','CW']"
      ],
      "metadata": {
        "id": "Ob8tBESxzENM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data_att[0], gt_att[0], test_size=0.25, random_state=42)\n",
        "print(names[0])\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "np.save(\"{}VANILLA/x_train\".format(PATH), x_train)\n",
        "np.save(\"{}VANILLA/x_test\".format(PATH), x_test)\n",
        "np.save(\"{}VANILLA/y_train\".format(PATH), y_train)\n",
        "np.save(\"{}VANILLA/y_test\".format(PATH), y_test)"
      ],
      "metadata": {
        "id": "nKfxda05x40d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adv_train, x_adv_test, y_train, y_test = train_test_split(data_att[0], gt_att[0], test_size=0.25, random_state=42)\n",
        "print(names[0])\n",
        "print(x_adv_train.shape)\n",
        "print(x_adv_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "np.save(\"{}FGSM/x_adv_train\".format(PATH), x_adv_train)\n",
        "np.save(\"{}FGSM/x_adv_test\".format(PATH), x_adv_test)\n",
        "np.save(\"{}FGSM/y_train\".format(PATH), y_train)\n",
        "np.save(\"{}FGSM/y_test\".format(PATH), y_test)"
      ],
      "metadata": {
        "id": "m8nT0OIrj0vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adv_train, x_adv_test, y_train, y_test = train_test_split(data_att[1], gt_att[1], test_size=0.25, random_state=42)\n",
        "print(names[1])\n",
        "print(x_adv_train.shape)\n",
        "print(x_adv_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "np.save(\"{}PGD/x_adv_train\".format(PATH), x_adv_train)\n",
        "np.save(\"{}PGD/x_adv_test\".format(PATH), x_adv_test)\n",
        "np.save(\"{}PGD/y_train\".format(PATH), y_train)\n",
        "np.save(\"{}PGD/y_test\".format(PATH), y_test)"
      ],
      "metadata": {
        "id": "PE_bcoYGzJDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adv_train, x_adv_test, y_train, y_test = train_test_split(data_att[2], gt_att[2], test_size=0.25, random_state=42)\n",
        "print(names[2])\n",
        "print(x_adv_train.shape)\n",
        "print(x_adv_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "np.save(\"{}I-FGSM/x_adv_train\".format(PATH), x_adv_train)\n",
        "np.save(\"{}I-FGSM/x_adv_test\".format(PATH), x_adv_test)\n",
        "np.save(\"{}I-FGSM/y_train\".format(PATH), y_train)\n",
        "np.save(\"{}I-FGSM/y_test\".format(PATH), y_test)"
      ],
      "metadata": {
        "id": "EXnQa2D8zQle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adv_train, x_adv_test, y_train, y_test = train_test_split(data_att[3], gt_att[3], test_size=0.25, random_state=42)\n",
        "print(names[3])\n",
        "print(x_adv_train.shape)\n",
        "print(x_adv_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "np.save(\"{}CW/x_adv_train\".format(PATH), x_adv_train)\n",
        "np.save(\"{}CW/x_adv_test\".format(PATH), x_adv_test)\n",
        "np.save(\"{}CW/y_train\".format(PATH), y_train)\n",
        "np.save(\"{}CW/y_test\".format(PATH), y_test)"
      ],
      "metadata": {
        "id": "S-JGMtUm4FNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "joRsUNRudSeo",
        "Op4vkukwdfD3",
        "iASTqptrdlIT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}