{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Of273bddN7p"
      },
      "source": [
        "#ADE-Net Baseline Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do3gCKSz4hAh"
      },
      "source": [
        "Code and Paper by: Nicholas Soucy and Dr. Salimeh Yasaei Sekeh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns_AGmkodPJ6"
      },
      "source": [
        "##Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joRsUNRudSeo"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jjrlbrndU1s",
        "outputId": "1c06210d-bf2d-44e8-d005-6071a093d973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import cohen_kappa_score as kappa\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "from sklearn import mixture\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import sklearn.preprocessing as sp\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import Tensor\n",
        "from torch import optim\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "import torch.utils.data as data_utils\n",
        "from scipy.sparse.linalg import LinearOperator as ScipyLinearOperator\n",
        "from scipy.sparse.linalg import eigsh\n",
        "from warnings import warn\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDxz7m5o6jw3"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyZ2l8Pm6kc5",
        "outputId": "bd6829ea-2908-4e71-f626-cef046b7fa7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PATH:  /Data/Kennedy_Space_Center/\n"
          ]
        }
      ],
      "source": [
        "# GLOBAL VARIABLES TO CHANGE\n",
        "\n",
        "# Batch Number\n",
        "batch = 512\n",
        "\n",
        "# Attack Stuff\n",
        "attack_types = ['FGSM', 'CW', 'PGD', 'I-FGSM', 'VANILA']\n",
        "num_attacks = 2\n",
        "\n",
        "# Number of Epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of classes\n",
        "class_num = 15\n",
        "cls = class_num\n",
        "\n",
        "dataset = \"KSC\"\n",
        "\n",
        "if dataset == \"KSC\":\n",
        "  PATH = \"Data/Kennedy_Space_Center/\"\n",
        "elif dataset == \"IP\":\n",
        "  PATH = \"Data/Indian_Pines/\"\n",
        "elif dataset == \"Houston\":\n",
        "  PATH = \"Data/Houston/\"\n",
        "\n",
        "print(\"PATH: \",PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iASTqptrdlIT"
      },
      "source": [
        "## Classification Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Xt7DmMdx7e"
      },
      "source": [
        "### Unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVP7JQLtB9d"
      },
      "source": [
        "#### Pytorch Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgF1LfkutD4Z"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self,in_shape,class_num,want_logits=True):\n",
        "        super().__init__()\n",
        "        # Define Variables\n",
        "        self.input_shape = in_shape\n",
        "        self.class_num = class_num\n",
        "        self.want_logits = want_logits\n",
        "        # Define Layers:\n",
        "        # NOTE: Change kernel size to be the size of the patch\n",
        "        self.Conv1 = nn.Conv2d(self.input_shape, 64,kernel_size=(1,1), stride=(1,1), padding='same', bias=False)\n",
        "        self.bn1 = nn.InstanceNorm1d(64)\n",
        "        self.Conv2 = nn.Conv2d(64, 128,kernel_size=(1,1), stride=(2,2), bias=False)\n",
        "        self.bn2 = nn.InstanceNorm1d(128)\n",
        "        self.Conv3 = nn.Conv2d(128, 256,kernel_size=(1,1), stride=(2,2), bias=False)\n",
        "        self.bn3 = nn.InstanceNorm1d(256)\n",
        "        self.deco1 = nn.ConvTranspose2d(256,256,kernel_size=(1,1), stride=(1,1), bias=False)\n",
        "        self.deco2 = nn.ConvTranspose2d(384,128,kernel_size=(1,1), stride=(1,1), bias=False)\n",
        "        self.deco3 = nn.ConvTranspose2d(192,class_num,kernel_size=(1,1), stride=(1,1))\n",
        "\n",
        "        # Define Activation functions:\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.Lrelu = nn.LeakyReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Layers: 4\n",
        "        Activation Functions:\n",
        "        RELU for first two layers\n",
        "        Sigmoid for third layer\n",
        "        Log Softmax for last layer\n",
        "        \"\"\"\n",
        "        # import pdb;pdb.set_trace()\n",
        "        x = self.Conv1(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn1(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        op1 = x\n",
        "\n",
        "        x = self.Conv2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        op2 = x\n",
        "\n",
        "        x = self.Conv3(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn3(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        op3 = x\n",
        "\n",
        "        x = self.deco1(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn3(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = torch.cat((x, op2), dim=1)\n",
        "\n",
        "        x = self.deco2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]/2),2)\n",
        "        # x = self.bn2(x)\n",
        "        # x = x.view(int(x.shape[0]),int(x.shape[1]*x.shape[2]),1,1)\n",
        "        x = self.Lrelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = torch.cat((x, op1), dim=1)\n",
        "\n",
        "        x = self.deco3(x)\n",
        "        x = torch.reshape(x, (x.shape[0], self.class_num))\n",
        "\n",
        "        if(self.want_logits):\n",
        "          return x\n",
        "        else:\n",
        "          x = self.softmax(x)\n",
        "          return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrzCjGT_z_6"
      },
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D78x7_cr_5S4"
      },
      "outputs": [],
      "source": [
        "def single_train_test(train_loader, test_loader, num_epochs=20, class_num=15, learning_rate=0.001):\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        in_shape = images.shape[1]\n",
        "        break\n",
        "\n",
        "    model = UNet(in_shape,class_num)\n",
        "    model.to(device)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train_running_loss = [] * num_epochs\n",
        "\n",
        "    running_acc = [] * num_epochs\n",
        "    \n",
        "    # Set parameters for testing\n",
        "    test_running_acc = [] * num_epochs\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        #Training Stuff\n",
        "        temp_loss = 0\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        #Testing Stuff\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "\n",
        "        ## training step\n",
        "\n",
        "        # Set networks to train\n",
        "        model.train()\n",
        "        \n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            # print(\"On batch: \",idx)\n",
        "\n",
        "            images = images.clone().detach().requires_grad_(True)\n",
        "            labels = labels.clone().detach().requires_grad_(True)\n",
        "            labels = labels.type(torch.LongTensor)  # casting to long\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Calculate gradient and update weights of discriminator network\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "\n",
        "            loss = ce(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            temp_loss += loss.detach().item()\n",
        "            total += labels.size(0)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ## Testing step\n",
        "\n",
        "        #Set each network to eval\n",
        "        model.eval()\n",
        "        for idx, (images, labels) in enumerate(test_loader):\n",
        "\n",
        "            #Get data\n",
        "            images = images.clone().detach().requires_grad_(True)\n",
        "            labels = labels.clone().detach().requires_grad_(True)\n",
        "            #Send data to device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            #output\n",
        "            output = model(images)\n",
        "            # a_labels = torch.tensor(np.array(tf.math.argmax(a_labels.cpu().clone().detach().numpy(), axis=1))).to(device)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        train_running_loss.append(temp_loss / idx)\n",
        "\n",
        "\n",
        "        if total == 0:\n",
        "            running_acc.append(0)\n",
        "        else: \n",
        "            running_acc.append((100 * correct / total))\n",
        "\n",
        "        if total_test == 0:\n",
        "            test_running_acc.append(0)\n",
        "        else:        \n",
        "            test_running_acc.append(100 * (correct_test/total_test))\n",
        "\n",
        "\n",
        "\t\t#Step Schedulers\n",
        "        scheduler.step()\n",
        "        \n",
        "        print('Epoch: %d | Train Loss: %.4f | Train Acc: %.4f | Testing Acc: %.4f' \\\n",
        "              % (epoch, train_running_loss[epoch], running_acc[epoch], test_running_acc[epoch]))\n",
        "\n",
        "    return model, train_running_loss, running_acc, test_running_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wmEZogc8tLz"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx7AA_x78vk4"
      },
      "source": [
        "### Load adversarial and vanilia data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnrzdVM9_Oad"
      },
      "outputs": [],
      "source": [
        "# Load vanilia and attacked datasets \n",
        "\n",
        "# vanilia\n",
        "vanilia_train_data = np.load('{}VANILLA/x_train.npy'.format(PATH))\n",
        "vanilia_train_labels = np.load('{}VANILLA/y_train.npy'.format(PATH))\n",
        "vanilia_test_data = np.load('{}VANILLA/x_test.npy'.format(PATH))\n",
        "vanilia_test_labels = np.load('{}VANILLA/y_test.npy'.format(PATH))\n",
        "\n",
        "\"My name is {}, I'm {}\".format(\"John\",36)\n",
        "\n",
        "print(\"Vanilia Data Shape\")\n",
        "print(vanilia_train_data.shape)\n",
        "print(vanilia_train_labels.shape)\n",
        "print(vanilia_test_data.shape)\n",
        "print(vanilia_test_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# fgsm\n",
        "fgsm_train_data = np.load('{}FGSM/x_adv_train.npy'.format(PATH))\n",
        "fgsm_train_labels = np.load('{}FGSM/y_train.npy'.format(PATH))\n",
        "fgsm_test_data = np.load('{}FGSM/x_adv_test.npy'.format(PATH))\n",
        "fgsm_test_labels = np.load('{}FGSM/y_test.npy'.format(PATH))\n",
        "\n",
        "print(\"FGSM Data Shape\")\n",
        "print(fgsm_train_data.shape)\n",
        "print(fgsm_train_labels.shape)\n",
        "print(fgsm_test_data.shape)\n",
        "print(fgsm_test_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#C_W\n",
        "\n",
        "cw_train_data = np.load('{}CW/x_adv_train.npy'.format(PATH))\n",
        "cw_train_labels = np.load('{}CW/y_train.npy'.format(PATH))\n",
        "cw_test_data = np.load('{}CW/x_adv_test.npy'.format(PATH))\n",
        "cw_test_labels = np.load('{}CW/y_test.npy'.format(PATH))\n",
        "\n",
        "print(\"CW Data Shape\")\n",
        "print(cw_train_data.shape)\n",
        "print(cw_train_labels.shape)\n",
        "print(cw_test_data.shape)\n",
        "print(cw_test_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "#pgd\n",
        "\n",
        "pgd_train_data = np.load('{}PGD/x_adv_train.npy'.format(PATH))\n",
        "pgd_train_labels = np.load('{}PGD/y_train.npy'.format(PATH))\n",
        "pgd_test_data = np.load('{}PGD/x_adv_test.npy'.format(PATH))\n",
        "pgd_test_labels = np.load('{}PGD/y_test.npy'.format(PATH))\n",
        "\n",
        "print(\"PGD Data Shape\")\n",
        "print(pgd_train_data.shape)\n",
        "print(pgd_train_labels.shape)\n",
        "print(pgd_test_data.shape)\n",
        "print(pgd_test_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#ifgsm\n",
        "\n",
        "ifgsm_train_data = np.load('{}I-FGSM/x_adv_train.npy'.format(PATH))\n",
        "ifgsm_train_labels = np.load('{}I-FGSM/y_train.npy'.format(PATH))\n",
        "ifgsm_test_data = np.load('{}I-FGSM/x_adv_test.npy'.format(PATH))\n",
        "ifgsm_test_labels = np.load('{}I-FGSM/y_test.npy'.format(PATH))\n",
        "\n",
        "print(\"I-FGSM Data Shape\")\n",
        "print(ifgsm_train_data.shape)\n",
        "print(ifgsm_train_labels.shape)\n",
        "print(ifgsm_test_data.shape)\n",
        "print(ifgsm_test_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "names = []\n",
        "\n",
        "# Create master arrays\n",
        "x_train = [None] * num_attacks\n",
        "x_test = [None] * num_attacks\n",
        "y_train = [None] * num_attacks\n",
        "y_test = [None] * num_attacks\n",
        "\n",
        "# Merge data and labels\n",
        "if num_attacks > 0:\n",
        "\tx_train[0] = fgsm_train_data\n",
        "\ty_train[0] = fgsm_train_labels\n",
        "\tx_test[0] = fgsm_test_data\n",
        "\ty_test[0] = fgsm_test_labels\n",
        "if num_attacks > 1:\n",
        "\tx_train[1] = cw_train_data\n",
        "\ty_train[1] = cw_train_labels\n",
        "\tx_test[1] = cw_test_data\n",
        "\ty_test[1] = cw_test_labels\n",
        "if num_attacks > 2:\n",
        "\tx_train[2] = pgd_train_data\n",
        "\ty_train[2] = pgd_train_labels\n",
        "\tx_test[2] = pgd_test_data\n",
        "\ty_test[2] = pgd_test_labels\n",
        "if num_attacks > 3:\n",
        "\tx_train[3] = ifgsm_train_data\n",
        "\ty_train[3] = ifgsm_train_labels\n",
        "\tx_test[3] = ifgsm_test_data\n",
        "\ty_test[3] = ifgsm_test_labels\n",
        "if num_attacks > 4:\n",
        "\tx_train[4] = vanilia_train_data\n",
        "\ty_train[4] = vanilia_train_labels\n",
        "\tx_test[4] = vanilia_test_data\n",
        "\ty_test[4] = vanilia_test_labels\n",
        "\n",
        "\n",
        "# Get names and print\n",
        "print(\"Attacks Used This Run\")\n",
        "for i in range(num_attacks):\n",
        "    names.append(attack_types[i])\n",
        "    print(names[i])\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Show attacked data shape\n",
        "\n",
        "\n",
        "print(\"Combined Data Shape\")\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(x_test))\n",
        "print(\"Combined Class Label Shape\")\n",
        "print(np.shape(y_train))\n",
        "print(np.shape(y_test))\n",
        "print(\"Combined Attack Label Shape\")\n",
        "print(np.shape(a_train))\n",
        "print(np.shape(a_test))\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o54sTGhl_axB"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLM1r8uh_a7r"
      },
      "source": [
        "### Training Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktohvgOS_eHp"
      },
      "outputs": [],
      "source": [
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "def unison_shuffled_copies3(a, b, c):\n",
        "    assert len(a) == len(b)\n",
        "    assert len(a) == len(c)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p], c[p]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrCNUWM8_gmT"
      },
      "outputs": [],
      "source": [
        "# get full data and shuffle\n",
        "if num_attacks == 2:\n",
        "\tgt = np.concatenate((np.array(y_train[0]), np.array(y_train[1])))\n",
        "\tdata = np.concatenate((np.array(x_train[0]), np.array(x_train[1])))\n",
        "elif num_attacks == 3:\n",
        "\tgt = np.concatenate((np.array(y_train[0]), np.array(y_train[1]), np.array(y_train[2])))\n",
        "\tdata = np.concatenate((np.array(x_train[0]), np.array(x_train[1]), np.array(x_train[2])))\n",
        "elif num_attacks == 4:\n",
        "\tgt = np.concatenate((np.array(y_train[0]), np.array(y_train[1]), np.array(y_train[2]), np.array(y_train[3])))\n",
        "\tdata = np.concatenate((np.array(x_train[0]), np.array(x_train[1]), np.array(x_train[2]), np.array(x_train[3])))\n",
        "elif num_attacks == 5:\n",
        "\tgt = np.concatenate((np.array(y_train[0]), np.array(y_train[1]), np.array(y_train[2]), np.array(y_train[3]), np.array(y_train[4])))\n",
        "\tdata = np.concatenate((np.array(x_train[0]), np.array(x_train[1]), np.array(x_train[2]), np.array(x_train[3]), np.array(x_train[4])))\n",
        "\n",
        "\n",
        "\n",
        "data, gt = unison_shuffled_copies(data, gt)\n",
        "\n",
        "# convert to Tensors\n",
        "gt = torch.Tensor(gt)\n",
        "data = torch.Tensor(data)\n",
        "\n",
        "# print(np.shape(gt))\n",
        "# print(np.shape(data))\n",
        "\n",
        "# create datasets\n",
        "dataset = data_utils.TensorDataset(data, gt)\n",
        "train_loader = data_utils.DataLoader(dataset, batch_size=batch, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "# get full data and shuffle\n",
        "if num_attacks == 2:\n",
        "\tgt_t = np.concatenate((np.array(y_test[0]), np.array(y_test[1])))\n",
        "\tdata_t = np.concatenate((np.array(x_test[0]), np.array(x_test[1])))\n",
        "elif num_attacks == 3:\n",
        "\tgt_t = np.concatenate((np.array(y_test[0]), np.array(y_test[1]), np.array(y_test[2])))\n",
        "\tdata_t = np.concatenate((np.array(x_test[0]), np.array(x_test[1]), np.array(x_test[2])))\n",
        "elif num_attacks == 4:\n",
        "\tgt_t = np.concatenate((np.array(y_test[0]), np.array(y_test[1]), np.array(y_test[2]), np.array(y_test[3])))\n",
        "\tdata_t = np.concatenate((np.array(x_test[0]), np.array(x_test[1]), np.array(x_test[2]), np.array(x_test[3])))\n",
        "elif num_attacks == 5:\n",
        "\tgt_t = np.concatenate((np.array(y_test[0]), np.array(y_test[1]), np.array(y_test[2]), np.array(y_test[3]), np.array(y_test[4])))\n",
        "\tdata_t = np.concatenate((np.array(x_test[0]), np.array(x_test[1]), np.array(x_test[2]), np.array(x_test[3]), np.array(x_test[4])))\n",
        "\n",
        "\n",
        "\n",
        "data_t, gt_t = unison_shuffled_copies(data_t, gt_t)\n",
        "\n",
        "# convert to Tensors\n",
        "gt_t = torch.Tensor(gt_t)\n",
        "data_t = torch.Tensor(data_t)\n",
        "\n",
        "# print(np.shape(gt))\n",
        "# print(np.shape(data))\n",
        "\n",
        "# create datasets\n",
        "dataset_t = data_utils.TensorDataset(data_t, gt_t)\n",
        "test_loader = data_utils.DataLoader(dataset_t, batch_size=batch, shuffle=False, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gjzOnRX_rq4"
      },
      "source": [
        "### Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GZ8dy-E_tan"
      },
      "outputs": [],
      "source": [
        "model, train_running_loss, running_acc, test_running_acc = single_train_test(train_loader, test_loader, num_epochs=num_epochs, class_num=class_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtcGaTeAAD7c"
      },
      "source": [
        "## Reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVnS0hdeAHgC"
      },
      "outputs": [],
      "source": [
        "print(\"Training Accuracy: \",max(running_acc))\n",
        "print(\"Testing Accuracy: \",max(test_running_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPyXN47OAJeR"
      },
      "outputs": [],
      "source": [
        "plt.plot(running_acc)\n",
        "plt.title('Training Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig('output/Train_Acc.png', bbox_inches='tight', dpi=400)\n",
        "plt.clf()\n",
        "plt.cla()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpxK0rOIAMqm"
      },
      "outputs": [],
      "source": [
        "plt.plot(test_running_acc)\n",
        "plt.title('Testing Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig('output/Test_Acc.png', bbox_inches='tight', dpi=400)\n",
        "plt.clf()\n",
        "plt.cla()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgY_AsIWAP5m"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_running_loss)\n",
        "plt.title('Training Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig('output/Loss.png', bbox_inches='tight', dpi=400)\n",
        "plt.clf()\n",
        "plt.cla()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXp8YA9tAQXS"
      },
      "outputs": [],
      "source": [
        "np.save(\"output/train_running_loss\", train_running_loss)\n",
        "np.save(\"output/running_acc\", running_acc)\n",
        "np.save(\"output/test_running_acc\", test_running_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
